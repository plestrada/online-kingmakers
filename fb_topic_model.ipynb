{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2694e482-2839-4f03-b08c-2a9ac55d840d",
    "tags": []
   },
   "source": [
    "## Facebook posts - Topic Modelling\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "- Posts made within the campaign period (Feb 12 - May 13, 2019) are taken from senatorial candidates' facebook page \n",
    "- Posts are translated to English\n",
    "- Posts containing words pertaining to election campaign activities are removed to focus on posts related to platforms and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "371ee24f-fa3e-445e-ad86-e0c564dbf680",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Arjoselle\n",
      "[nltk_data]     Ortiz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1c5f9da0-e6da-4439-a918-572f25fa0608",
    "tags": []
   },
   "source": [
    "### 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "7425d52a-c42d-4834-a665-f48f23eed8b8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('fb_posts_translated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Drop irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "a82fa16e-ba15-4d25-bef2-db51a214df1a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate</th>\n",
       "      <th>Translated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bong Go</td>\n",
       "      <td>Happy Mother's Day From Tatay Digong!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bong Go</td>\n",
       "      <td>Thank you so much to all of you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bong Go</td>\n",
       "      <td>KUYA BONG GO, SATURDAY HOMETOWN DAVAO CITY FOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bong Go</td>\n",
       "      <td>Vhong Navarro has shown support for Kuya Bong ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bong Go</td>\n",
       "      <td>Bong Go at Cogon Public Market, Cagayan de Oro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Candidate                                    Translated_text\n",
       "0   Bong Go              Happy Mother's Day From Tatay Digong!\n",
       "1   Bong Go                   Thank you so much to all of you!\n",
       "2   Bong Go  KUYA BONG GO, SATURDAY HOMETOWN DAVAO CITY FOR...\n",
       "3   Bong Go  Vhong Navarro has shown support for Kuya Bong ...\n",
       "4   Bong Go  Bong Go at Cogon Public Market, Cagayan de Oro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2664, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "f70c223d-249c-4b1f-9eb5-0362f109a314",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Happy Mother's Day From Tatay Digong!\"\n"
     ]
    }
   ],
   "source": [
    "pprint(df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Drop rows related to election campaign activities to focus on posts related to platforms and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "f3d35cbb-30f7-43f9-8956-b627e377d8aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(861, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'vote' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'thank' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'election' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'elections' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'senatorial' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'senator' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'ballot' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'candidate' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'campaign' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'rally' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'senado' in str(x).lower() else x)\n",
    "df.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'balota' in str(x).lower() else x)\n",
    "\n",
    "df.dropna(subset=['Translated_text'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c062a20-13ec-48dc-bc89-a3fd1956cd4d",
    "tags": []
   },
   "source": [
    "### 2. Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "21d27e3c-dcb8-4018-b198-c3c361546d2a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "839"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.Translated_text.values.tolist()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1621145c-5906-41f4-a273-30989174746c",
    "tags": []
   },
   "source": [
    "#### a. Prepare stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "1f0519dd-3207-46cd-86cb-7798e6dc3cf1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['thank', 'support', 'much', 'election', 'ballot', 'candidate', 'vote', 'campaign', 'ate', 'gracepoe', 'ating', 'cheldiokno', 'aking', 'nating', 'pampanga', 'watch', 'ng', 'avance', 'atty', 'live' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3a631c43-148c-4564-b1e2-e4f1a939c24c",
    "tags": []
   },
   "source": [
    "#### b. Tokenize words and Clean-up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "5ec1279e-b92b-4f35-b990-ab0c83881704",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'mother', 'day', 'from', 'tatay', 'digong']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "data_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "12c73726-e3b5-40ca-8383-630e0b5265c1",
    "tags": []
   },
   "source": [
    "#### c. Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "e91643d0-d578-458e-bbb4-4f247cadbbbd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'mother', 'day', 'from', 'tatay', 'digong']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cf8d6225-5afa-41b3-a7e7-dce74a601226",
    "tags": []
   },
   "source": [
    "#### d. Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "441fea23-8a7a-41fc-8f99-cfdb8fb13bbe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        if 'nate' in texts_out:\n",
    "            print(sent)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "0737b01b-3c95-4514-ba0c-b4703c4c4f66",
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ','VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "531ece8c-ca1f-48f0-9546-3f4da8a8081a",
    "tags": []
   },
   "source": [
    "#### e. Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "148c5d97-1ffd-4e0b-af70-a36ba0b3e57d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "3871d048-7a92-4084-b8f9-c39e21d75573",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c33d0185-6768-4526-ba01-9edd19e9d2b8",
    "tags": []
   },
   "source": [
    "### 3. Build the Model\n",
    "\n",
    "Using Mallet LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "2de2e42c-657b-4699-aa83-62cc7431bc3d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#C:\\Users\\Arjoselle Ortiz\\Documents\\DS Classes\\Eskwelabs\\Capstone\\mallet-2.0.8\\mallet-2.0.8\\bin\n",
    "os.environ.update({'MALLET_HOME':r'C:/new_mallet/mallet-2.0.8/'})\n",
    "mallet_path = 'C:\\\\new_mallet\\\\mallet-2.0.8\\\\bin\\\\mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=9, id2word=id2word, random_seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "8494fffc-26a5-4496-94a3-c6caf4844fd2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('time', 0.04173106646058733),\n",
      "   ('today', 0.023183925811437404),\n",
      "   ('long', 0.023183925811437404),\n",
      "   ('poor', 0.023183925811437404),\n",
      "   ('water', 0.02009273570324575),\n",
      "   ('official', 0.02009273570324575),\n",
      "   ('system', 0.017001545595054096),\n",
      "   ('countryman', 0.015455950540958269),\n",
      "   ('woman', 0.013910355486862442),\n",
      "   ('disaster', 0.013910355486862442)]),\n",
      " (1,\n",
      "  [('country', 0.08494783904619971),\n",
      "   ('public', 0.040238450074515646),\n",
      "   ('medicine', 0.02533532041728763),\n",
      "   ('back', 0.022354694485842028),\n",
      "   ('law', 0.022354694485842028),\n",
      "   ('pay', 0.01788375558867362),\n",
      "   ('company', 0.01788375558867362),\n",
      "   ('war', 0.013412816691505217),\n",
      "   ('incentive', 0.013412816691505217),\n",
      "   ('list', 0.011922503725782414)]),\n",
      " (2,\n",
      "  [('good', 0.08229813664596274),\n",
      "   ('price', 0.04813664596273292),\n",
      "   ('market', 0.04813664596273292),\n",
      "   ('sell', 0.029503105590062112),\n",
      "   ('year', 0.029503105590062112),\n",
      "   ('talk', 0.027950310559006212),\n",
      "   ('farmer', 0.018633540372670808),\n",
      "   ('town', 0.015527950310559006),\n",
      "   ('weight', 0.015527950310559006),\n",
      "   ('salamat', 0.012422360248447204)]),\n",
      " (3,\n",
      "  [('people', 0.08708272859216255),\n",
      "   ('friend', 0.02902757619738752),\n",
      "   ('serve', 0.026124818577648767),\n",
      "   ('life', 0.026124818577648767),\n",
      "   ('business', 0.023222060957910014),\n",
      "   ('small', 0.02177068214804064),\n",
      "   ('love', 0.02177068214804064),\n",
      "   ('mother', 0.02177068214804064),\n",
      "   ('morning', 0.018867924528301886),\n",
      "   ('full', 0.015965166908563134)]),\n",
      " (4,\n",
      "  [('worker', 0.03814262023217247),\n",
      "   ('fight', 0.03150912106135987),\n",
      "   ('tax', 0.029850746268656716),\n",
      "   ('include', 0.028192371475953566),\n",
      "   ('job', 0.024875621890547265),\n",
      "   ('child', 0.01824212271973466),\n",
      "   ('member', 0.01824212271973466),\n",
      "   ('rise', 0.01824212271973466),\n",
      "   ('noon', 0.014925373134328358),\n",
      "   ('low', 0.014925373134328358)]),\n",
      " (5,\n",
      "  [('make', 0.06633499170812604),\n",
      "   ('sakit', 0.03150912106135987),\n",
      "   ('service', 0.026533996683250415),\n",
      "   ('heart', 0.02321724709784411),\n",
      "   ('student', 0.02155887230514096),\n",
      "   ('provide', 0.01990049751243781),\n",
      "   ('high', 0.01824212271973466),\n",
      "   ('leader', 0.01824212271973466),\n",
      "   ('day', 0.01658374792703151),\n",
      "   ('start', 0.013266998341625208)]),\n",
      " (6,\n",
      "  [('give', 0.03803486529318542),\n",
      "   ('local', 0.03169572107765452),\n",
      "   ('meet', 0.025356576862123614),\n",
      "   ('city', 0.02377179080824089),\n",
      "   ('political', 0.022187004754358162),\n",
      "   ('issue', 0.017432646592709985),\n",
      "   ('development', 0.017432646592709985),\n",
      "   ('health', 0.01584786053882726),\n",
      "   ('warm', 0.014263074484944533),\n",
      "   ('benefit', 0.014263074484944533)]),\n",
      " (7,\n",
      "  [('family', 0.04492753623188406),\n",
      "   ('work', 0.036231884057971016),\n",
      "   ('program', 0.03188405797101449),\n",
      "   ('increase', 0.02608695652173913),\n",
      "   ('filipino', 0.02608695652173913),\n",
      "   ('money', 0.02608695652173913),\n",
      "   ('income', 0.02463768115942029),\n",
      "   ('cheap', 0.021739130434782608),\n",
      "   ('promote', 0.020289855072463767),\n",
      "   ('visit', 0.017391304347826087)]),\n",
      " (8,\n",
      "  [('government', 0.05206738131699847),\n",
      "   ('continue', 0.03828483920367534),\n",
      "   ('call', 0.019908116385911178),\n",
      "   ('chinese', 0.018376722817764167),\n",
      "   ('sea', 0.018376722817764167),\n",
      "   ('part', 0.016845329249617153),\n",
      "   ('philippine', 0.016845329249617153),\n",
      "   ('number', 0.013782542113323124),\n",
      "   ('interest', 0.013782542113323124),\n",
      "   ('true', 0.013782542113323124)])]\n",
      "\n",
      "Coherence Score:  0.5027183684653616\n"
     ]
    }
   ],
   "source": [
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1b563d7e-cc04-46a6-b9d2-6d39bf9301e2"
   },
   "source": [
    "### 3. Tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "442624f9-d081-483a-b63e-d4e6b51ef9e6"
   },
   "source": [
    "#### a. Optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b4b9a04a-b810-4854-b790-e59b0dff6ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(num_topics)\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word,\n",
    "                                                random_seed=123)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "start1 = 4 \n",
    "limit1 = 13\n",
    "step1 = 1\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=start1, limit=limit1, step=step1)\n",
    "\n",
    "x = range(start1, limit1, step1)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a6695aa1-23e9-48c6-9885-0db0c2f7d5bf"
   },
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll choose 9 topics as it has high coherence score and \n",
    "seems a reasonable number of topics to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Build optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f9fd2ff9-9ea9-4d59-bf41-916c962d4930"
   },
   "outputs": [],
   "source": [
    "optimal_model = model_list[-4] # Number of Topics = 9\n",
    "\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3233b0bc-eb73-4ad4-8543-e82d4f37a7cc"
   },
   "outputs": [],
   "source": [
    "# Convert LDA Mallet to normal LDA since pyLDAvis only takes the latter for visualization\n",
    "optimal_model2 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(optimal_model)\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(optimal_model2, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "97602dcf-fe20-4c44-ac1e-c01b17dcc813"
   },
   "source": [
    "#### b. Dominant topic and its percentage contribution in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(optimal_model.read_doctopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3a51e75b-eb9c-47a8-87c3-ea5069d44ac7"
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "abfed287-0604-46f6-8cb6-59cbc802fa5d"
   },
   "outputs": [],
   "source": [
    "df_dominant_topic['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 0].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 1].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 2].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 3].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 4].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 5].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 6].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 7].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 7].sort_values('Topic_Perc_Contrib', ascending=False).iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6228a429-972e-48f1-aab0-eb6d1f7e8eac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 0]['Text'].values # Addressing various issues - 261\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 1]['Text'].values # Medicine, War on Drugs, Foreign Investments - 66\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 2]['Text'].values # Price of goods - 79\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 3]['Text'].values # Hardworking Filipinos - 92\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 4]['Text'].values # Jobs/Labor - 76\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 5]['Text'].values # Health, Youth - 83\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 6]['Text'].values # Good Governance, Local Government programs - 69\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 7]['Text'].values # Small entrepreneurs/Workers - 49\n",
    "# df_dominant_topic[df_dominant_topic['Dominant_Topic'] == 8]['Text'].values # Sovereignity/Independence - 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b0ef3263-5534-4991-a28e-3d11861527a9"
   },
   "source": [
    "#### c. The most representative sentence for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7ccf307a-b456-4581-81d8-cf2eb5bc9766"
   },
   "outputs": [],
   "source": [
    " #Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Top words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a3ba318a-d031-4459-b69f-2cbeda96e2ea"
   },
   "outputs": [],
   "source": [
    "# Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = optimal_model.show_topics(formatted=False, num_words=15)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15,15), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=7, angle=.99, init='pca')# 13-D -> 2-D\n",
    "tsne_lda = tsne_model.fit_transform(list(optimal_model.load_document_topics())) # doc_topic is document-topic matrix from LDA or GuidedLDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_document_topics = optimal_model2.get_document_topics(corpus, minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_document_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0 = []\n",
    "topic1 = []\n",
    "topic2 = []\n",
    "topic3 = []\n",
    "topic4 = []\n",
    "topic5 = []\n",
    "topic6 = []\n",
    "topic7 = []\n",
    "topic8 = []\n",
    "\n",
    "for score in get_document_topics:\n",
    "    for col in score:\n",
    "        if col[0] == 0:\n",
    "            topic0.append(col[1])\n",
    "        elif col[0] == 1:\n",
    "            topic1.append(col[1])\n",
    "        elif col[0] == 2:\n",
    "            topic2.append(col[1])\n",
    "        elif col[0] == 3:\n",
    "            topic3.append(col[1])\n",
    "        elif col[0] == 4:\n",
    "            topic4.append(col[1])\n",
    "        elif col[0] == 5:\n",
    "            topic5.append(col[1])\n",
    "        elif col[0] == 6:\n",
    "            topic6.append(col[1])\n",
    "        elif col[0] == 7:\n",
    "            topic7.append(col[1])\n",
    "        elif col[0] == 8:\n",
    "            topic8.append(col[1])\n",
    "        elif col[0] == 9:\n",
    "            topic9.append(col[1])\n",
    "            \n",
    "            \n",
    "print(len(topic0), len(topic1), len(topic2))\n",
    "print(len(topic3), len(topic4), len(topic5))\n",
    "print(len(topic6), len(topic7), len(topic8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Topic 0'] = topic0\n",
    "df['Topic 1'] = topic1\n",
    "df['Topic 2'] = topic2\n",
    "df['Topic 3'] = topic3\n",
    "df['Topic 4'] = topic4\n",
    "df['Topic 5'] = topic5\n",
    "df['Topic 6'] = topic6\n",
    "df['Topic 7'] = topic7\n",
    "df['Topic 8'] = topic8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dominant Topic'] = df_dominant_topic['Dominant_Topic'].values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('fb_topic_model_9topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict topic for all posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv('fb_posts_translated.csv')\n",
    "# df_test.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'vote' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'thank' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'election' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'elections' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'senatorial' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'senator' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'ballot' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'candidate' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'campaign' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'rally' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'senado' in str(x).lower() else x)\n",
    "# df_test.Translated_text = df.Translated_text.apply(lambda x: np.NaN if 'balota' in str(x).lower() else x)\n",
    "# df_test.dropna(subset=['Translated_text'], inplace=True)\n",
    "# df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to list\n",
    "# data = df_test.Translated_text.iloc[500:1000].values.tolist()\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_words = list(sent_to_words(data))\n",
    "# data_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the bigram and trigram models\n",
    "# bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "# trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Stop Words\n",
    "# data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# # Form Bigrams\n",
    "# data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# # python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ','VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Dictionary\n",
    "# id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# # Create Corpus\n",
    "# texts = data_lemmatized\n",
    "\n",
    "# # Term Document Frequency\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# # View\n",
    "# print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predicted_results = pd.DataFrame(columns=['Topic0','Topic1','Topic2','Topic3','Topic4','Topic5','Topic6','Topic7','Topic8'])\n",
    "\n",
    "# for index, text in enumerate(data):\n",
    "#     x = optimal_model[corpus[index]]\n",
    "#     predicted_results.loc[text]=x\n",
    "\n",
    "# predicted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_results[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "dc362dc6-59eb-4652-821b-ade690738d12",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
